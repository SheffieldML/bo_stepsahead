\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{osborne_gaussian_2009}
\citation{Marchant*Ramos*Sanner*2014}
\citation{Ginsbourger2009}
\citation{Azimi2012,Azimi2011}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction\relax }{section.1}{}}
\citation{Borodin*Rains*2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A Bayesian network describing the $n$-step lookahead problem. The shaded node ($\mathcal  {D}_0$) is known, and the diamond node ($x_{\ast }$) is the current decision variable. All $y$ nodes are correlated with one another under the \textsc  {gp}\xspace  model. }}{2}{figure.1}}
\newlabel{fig:bayes_net}{{1}{2}{ A Bayesian network describing the $n$-step lookahead problem. The shaded node ($\data _0$) is known, and the diamond node ($\xst $) is the current decision variable. All $y$ nodes are correlated with one another under the \gp model. \relax }{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Look-Ahead through Stochastic Simulation and Expected-loss Search (perhaps other title is better?)}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Conditional DPPs for step ahead locations simulation}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Expectation Propagation to compute the Expected Loss}{2}{subsection.2.2}}
\newlabel{eq:expected_loss}{{2}{2}{Expectation Propagation to compute the Expected Loss\relax }{equation.2.2}{}}
\citation{Cunningham*Hennig*Lacoste-Julien_2011}
\citation{Cunningham*Hennig*Lacoste-Julien_2011}
\bibstyle{plain}
\bibdata{bib_glasses}
\bibcite{Azimi2011}{{1}{}{{}}{{}}}
\bibcite{Azimi2012}{{2}{}{{}}{{}}}
\bibcite{Borodin*Rains*2005}{{3}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Left: sample from a k-\textsc  {dpp}\xspace  (k=50) for a SE kernel with length-scale 0.5. Right: sample from a k-\textsc  {dpp}\xspace  (k=50) conditional to $x_1$ (black dot) being in the selected set. }}{3}{figure.2}}
\newlabel{fig1}{{2}{3}{Left: sample from a k-\dpp (k=50) for a SE kernel with length-scale 0.5. Right: sample from a k-\dpp (k=50) conditional to $x_1$ (black dot) being in the selected set. \relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Approximating the Expected loss function}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Algorithm}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{3}{section.4}}
\bibcite{Cunningham*Hennig*Lacoste-Julien_2011}{{4}{}{{}}{{}}}
\bibcite{Ginsbourger2009}{{5}{}{{}}{{}}}
\bibcite{Marchant*Ramos*Sanner*2014}{{6}{}{{}}{{}}}
\bibcite{osborne_gaussian_2009}{{7}{}{{}}{{}}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Decission process of the \textsc  {glasses}\xspace  algorithm.}}{4}{algorithm.1}}
\newlabel{alg:glasses}{{1}{4}{Algorithm\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {S1}Proofs}{5}{section.1}}
\newlabel{eq:term1}{{S.1}{5}{Proofs\relax }{equation.1.1}{}}
\newlabel{eq:term2}{{1}{5}{Proofs\relax }{equation.1.1}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
