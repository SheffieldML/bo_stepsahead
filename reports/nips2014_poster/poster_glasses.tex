\documentclass[15pt,serif,mathserif,final]{beamer}
\mode<presentation>{\usetheme{Lankton}}
\usepackage{amsmath,amsfonts,amssymb,pxfonts,eulervm,xspace}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{fancybox}
\usetikzlibrary{bayesnet}
\tikzstyle{connect}=[-latex]
\tikzstyle{allconnected}=[line width=0.1cm]



\graphicspath{{./}{../../../bo_stepsahead/reports/diagrams/}}
\newcommand{\todiagrams}{../diagrams/}
\usepackage[orientation=portrait,size=custom,width=70,height=40,scale=.6,debug]{beamerposter}
%\usepackage{xspace}

%\newcommand{\acr}[1]{\textsc{#1}\xspace}
%\newcommand{\gp}{\acr{gp}}
%\newcommand{\dpp}{\acr{dpp}}
%\newcommand{\us}{\acr{glasses}}
%\newcommand{\direct}{\acr{direct}}
%\newcommand{\lbfgs}{\acr{l-bfgs}}
%\newcommand{\map}{\acr{map}}
%\newcommand{\ep}{\acr{ep}}
%\newcommand{\bo}{\acr{bo}}
%\newcommand{\mpi}{\acr{mpi}}
%\newcommand{\el}{\acr{el}}
%\newcommand{\lcb}{\acr{gp-lcb}}


\newcommand{\vp}{\vec{\phi}}
\newcommand{\vmu}{\vec{\mu}}
\newcommand{\vf}{\vec{f}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\E}{\mathbb{E}}
%\newcommand{\dataVector}{\textbf{y}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\vk}{\vec{k}}
\newcommand{\vL}{\vec{\Lambda}}
\newcommand{\xmin}{x_{\min}}
\newcommand{\pmin}{p_{\min}}
\newcommand{\fmin}{f_{\min}}
\newcommand{\pfmin}{p_{f_{\min}}}
\renewcommand{\vec}{\boldsymbol}
\newcommand{\fun}[1]{\mathsf{#1}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Id}{\vec{I}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\future}{\mathcal{F}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\argmin}{\operatorname*{arg\: min}}
\newcommand{\argmax}{\operatorname*{arg\: max}}
\newcommand{\chol}{\operatorname{\mathsf{C}}}

\newcommand{\xst}{x_{\ast}}
\newcommand{\yst}{y_{\ast}}


\graphicspath{{../diagrams/}}
\input{../tex_inputs/notationDef.tex}
\input{../tex_inputs/definitions.tex}

%\newcommand{\IR}{\mathbb{R}}
%\def\bbbr{{\rm I\!R}}
%-- Header and footer information ----------------------------------
\newcommand{\footleft}{Funded by the BRIC-BBSRC  project No BB/K011197/1.}
\newcommand{\footright}{$^*$\texttt{\{j.h.gonzalez\}@sheffield.ac.uk} }
\title{\Huge{GLASSES: Relieving The Myopia Of Bayesian Optimisation}}
\author{Javier Gonz\'{a}lez$^{1,*}$ \quad Michael Osborne$^{2}$ \quad  Neil D. Lawrence$^{1}$}
\institute{$^1$University of Sheffield, $^2$University of Oxford}
%-------------------------------------------------------------------
\definecolor{mycolor}{rgb}{0, 0, 1}
\definecolor{trolleygrey}{rgb}{0.5, 0.5, 0.5}
%-- Main Document --------------------------------------------------
\begin{document}
\begin{frame}{}
  \begin{columns}[t]

    %-- Column 1 ---------------------------------------------------
    \begin{column}{0.32\linewidth}

      %-- Block 1-1
      \begin{block}{Motivation and Summary}





\begin{itemize}
\item We present \textcolor{mycolor}{GLASSES}: \emph{Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search.} 
\item GLASSES is a \textcolor{mycolor}{non-myopic loss for Bayesian Optimisation} that permits the consideration of dozens of evaluations into the future.
\item We show that \textcolor{mycolor}{the far-horizon planning thus enabled leads to substantive performance gains} in empirical tests. 
\item Code available in the \textcolor{mycolor}{GPyOpt} package (\texttt{https://github.com/SheffieldML/GPyOpt}).
\end{itemize}
      \end{block}

      %-- Block 1-2
      
\begin{block}{Global optimisation problems}
Let $f: {\mathcal X} \to \Re$ be well behaved function defined on a compact subset ${\mathcal X} \subseteq \Re^{\inputDim}$. Find 
$$\latentVector_{M} = \arg \min_{\latentVector \in {\mathcal X}} f(\latentVector).$$ 
\textcolor{mycolor}{$f$ is a \emph{black-box}}: only evaluations of the type $\dataScalar_i = f(\latentVector_i) + \noiseScalar_i$, with $\noiseScalar_i \sim\mathcal{N}(0,\dataStd^2)$ are  available.
\end{block}      
      
\begin{block}{Bayesian optimisation with a myopic expected loss}
\begin{figure}
  \centerline{
    \begin{tikzpicture}[thick]
\draw (0,0) -- (6,0);
\draw (0,-.5) -- (0,.5);
\draw (6,-.5) -- (6,.5);
\draw[red] (2.6,-.3) -- (3.4,.3);
\draw[red] (2.6,.3) -- (3.4,.-.3);
\draw (9,0) -- (15,0);
\draw (9,-.5) -- (9,.5);
\draw (15,-.5) -- (15,.5);
\draw[red] (11.6,-.3) -- (12.4,.3);
\draw[red] (11.6,.3) -- (12.4,.-.3);
\draw[blue] (10.1,-.3) -- (10.9,.3);
\draw[blue] (10.1,.3) -- (10.9,.-.3);
    \end{tikzpicture}
  }
  \caption{Two evaluations: if the first evaluation is made myopically, the second must be sub-optimal.}
\end{figure}
      
\begin{itemize}
%\item \textcolor{mycolor}{Bayesian Optimization (BO)}: Heuristic strategy to collect $\latentVector_1,\dots,\latentVector_n$ of $f$, such that the the minimum of $f$ is evaluated as soon as possible.
\item $\dataSet_0 = \{(\latentVector_i,\dataScalar_i)\}_{i=1}^N = (\latentMatrix_0,\dataVector_0)$: Available dataset. 
\item $p(f) = \mathcal{GP}(\mu; k)$: Gaussian process (GP) using $\dataSet_0.$
\item $\I_0$: conjunction of $\dataSet_0$, the model parameters and the model likelihood type.
\item $\eta = \min \{\dataVector_0\}$: current best found value.
\item One remaining evaluation before we need to report our inferred location of the minimum.
\end{itemize}
      
The \textcolor{mycolor}{\emph{loss}} of evaluating $f$ this last time at $\latentVector_*$ assuming it is returning $y_*$ is
$$\lambda(y_*)\triangleq \left\{ \begin{array}{lcl}
y_*;             & \mbox{if}  &  y_* \leq \eta \\
 \eta; & \mbox{if}  & y_* > \eta. \\
\end{array}
\right.$$
The \textcolor{mycolor}{\emph{expectation of the loss}}:
$$\ \Lambda_1(\latentVector_*| \I_0) \triangleq \E[ \min (y_*,\eta)]= \int \lambda(y_*)p(y_* |\latentVector_*,\I_0)\ud y_* =  \eta +(\mu  - \eta) \Phi (\eta ; \mu, \sigma^2) - \sigma^2 \mathcal{N} (\eta, \mu, \sigma^2)$$
\begin{itemize}
\item We have abbreviated $\sigma^2(y_*|\I_0)$ as $\sigma^2$ and $\mu(y_{*}|\I_0)$ as $\mu$. 
\item The subscript in $\Lambda$ refers to the fact that we are considering one future evaluation.  
\item The next evaluation is located  where $\Lambda_1(\latentVector_*| \I_0) $ gives the minimum value [1].
\end{itemize}

 \begin{center}
 \textcolor{red}{\textbf{$\Lambda_1(\latentVector_*| \I_0)$ is myopic: doesn't take into account the number of remaining evaluations.}}
\end{center}

     \end{block}

      %-- Block 1-3


    \end{column}%1

    %-- Column 2 ---------------------------------------------------
    \begin{column}{0.32\linewidth}

      %-- Block 2-3
\begin{block}{Ideal non-myopic expected loss}

\begin{figure}[t!]
\centering
\begin{tikzpicture}

    % first row
    \node[obs] (D0) {$\dataSet_0$};
    \node[latent, right=of D0, xshift=1.2cm] (D1) {$\dataSet_1$};
    \node[draw=none, right=of D1, xshift=1.2cm] (Ddots) {$\ldots$};
    \node[latent, right=of Ddots, xshift=1.2cm] (Dn) {$\dataSet_n$};

    % second row
    \node[det, below=of D0, xshift=1.2cm] (xst) {$\xst$};
    \node[latent, right=of xst, xshift=1.2cm] (x2) {$x_2$};
    \node[latent, right=of x2, xshift=4.8cm] (xn) {$x_n$};

    % third row
    \node[latent, below=of xst] (yst) {$\yst$};
    \node[latent, below=of x2] (y2) {$y_2$};
    \node[draw=none, right=of y2, xshift=1.2cm] (ydots) {$\ldots$};
    \node[latent, below=of xn] (yn) {$y_n$};

    % Connect the nodes
    \path 
        (D0) edge [connect] (D1)
        (D0) edge [connect] (xst)
        (xst) edge [connect] (yst)
        (xst) edge [connect] (D1)
        (yst) edge [connect, bend right=20] (D1)
        (yst) edge [allconnected] (y2)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (y2) edge [allconnected] (ydots)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (ydots) edge [allconnected] (yn)

        (Ddots) edge [connect] (Dn)
        (Dn) edge [connect] (xn)
        (xn) edge [connect] (yn)        
        ;
\end{tikzpicture}
\caption{
    A Bayesian network describing the $n$-step lookahead problem. }
\label{fig:bayes_net}
\end{figure}
      
The \textcolor{blue}{\emph{ideal long-sight}} loss is defined as:
$$\Lambda_n(\latentVector_*|\I_0 ) =  \int \lambda(y_n) \prod_{j=1}^{n}p(y_{j}|\latentVector_{j},\I_{j-1}) p(\latentVector_{j}|\I_{j-1}) \ud y_*\dots \ud y_n \ud\latentVector_2\dots \ud\latentVector_n$$
\begin{itemize}
\item $p(y_{j}|\latentVector_{j},\I_{j-1})= \mathcal{N} \left(y_{j};\mu(\latentVector_{j};\I_{j-1}),\sigma^2(\latentVector_{j}|\I_{j-1} ) \right)$: predictive distribution of the GP at $\latentVector_{j}$
\item $p(\latentVector_{j}|\I_{j-1}) = \delta \bigr(\latentVector_{j} - \arg \min_{\latentVector_* \in {\mathcal X}} \Lambda_{n-j+1}(\latentVector_*|\I_{j-1})\bigl)$: optimisation step required to obtain $\latentVector_{j}$.
\end{itemize}

 \begin{center}
 \textcolor{red}{\textbf{This long-sight loss is extremely expensive to compute}}
 \end{center}

\end{block}


      %-- Block 2-2
\begin{block}{GLASSES}

\begin{figure}[t!]
\centering
\begin{tikzpicture}

    % first row
    \node[obs] (D0) {$\dataSet_0$};

    % second row
    \node[det, below=of D0, xshift=1.2cm] (xst) {$\xst$};
    \node[latent, right=of xst, xshift=1.2cm] (x2) {$x_2$};
    \node[draw=none, right=of x2, xshift=1.2cm] (xdots) {$\ldots$};
    \node[latent, right=of x2, xshift=4.8cm] (xn) {$x_n$};

    % third row
    \node[latent, below=of xst] (yst) {$\yst$};
    \node[latent, below=of x2] (y2) {$y_2$};
    \node[draw=none, right=of y2, xshift=1.2cm] (ydots) {$\ldots$};
    \node[latent, below=of xn] (yn) {$y_n$};

    % Connect the nodes
    \path 
        (D0) edge [connect] (xst)
        (xst) edge [connect] (yst)
        (yst) edge [allconnected] (y2)

        (D0) edge [connect, bend left=20] (x2)
        (D0) edge [connect, bend left=10] (xn)

        (xst) edge [allconnected] (x2)
        (x2) edge [allconnected] (x2)
        (x2) edge [allconnected] (xdots)
        (xdots) edge [allconnected] (xn)

        (x2) edge [connect] (y2)
        (y2) edge [allconnected] (ydots)

        (x2) edge [connect] (y2)
        (ydots) edge [allconnected] (yn)

        (xn) edge [connect] (yn)        
        ;
\end{tikzpicture}
\caption{
    A Bayesian network describing our approximation to the $n$-step lookahead problem.  Compare with the figure above: the sparser structure renders our approximation computationally tractable. }
\label{fig:bayes_net_glasses}
\end{figure}

Take $\future_{n}(\latentVector_*)$ is an oracle function able to predict the $n$ future  locations starting at $\latentVector_*$:
\begin{equation}\nonumber
\Lambda_n \bigl(\latentVector_* \mid \I_0, \future_{n}(\latentVector_*) \bigr)  =\E [\min (\dataVector,\eta)]  =  \eta\int_{\IR^n} \prod_{i=1}^nh_i(\dataVector) \N(\dataVector; \mu, \Sigma) \ud \dataVector + \sum_{j=1}^n  \int_{\IR^n} \dataScalar_j \prod_{i=1}^n t_{j,i}(\dataVector) \N(\dataVector; \mu, \Sigma) \ud \dataVector,
\end{equation}
 where  $h_i(\dataVector) = \mathbb{I}\{\dataScalar_i>\eta\}$ and $t_{j,i}(\dataVector) = \mathbb{I}\{\dataScalar_j \leq\eta\}$ if $i=j$ and 
$t_{j,i}(\dataVector) =\mathbb{I}\{ 0 \leq \dataScalar_i-\dataScalar_j \}$ otherwise.

 \begin{itemize}
 \item $\Lambda_n \bigl(\latentVector_* \mid \I_0, \future_{n}(\latentVector_*) \bigr)$ can be computed with Expectation Propagation [2].
 \item A batch BO method [3] is used as a surrogate for $\future_{n}(\latentVector_*)$. 
 \item $\Lambda_n \bigl(\latentVector_* \mid \I_0, \future_{n}(\latentVector_*) \bigr)$ is optimised using a  gradient-free method (DIRECT).
 \end{itemize}
 
 \begin{center}
 \textcolor{red}{\textbf{First non-myopic loss able to take into account dozens of future evaluations}}
 \end{center}
 
\end{block}





    \end{column}%2

    %-- Column 3 ---------------------------------------------------
    \begin{column}{0.32\linewidth}

      %-- Block 3-1
      \begin{block}{Results}


\begin{figure}[t!]
\begin{tabular}{cccc}
      {\includegraphics[width=45mm]{1_ahead.pdf}} &
      {\includegraphics[width=47mm]{2_ahead.pdf}}  &
      {\includegraphics[width=45mm]{5_ahead.pdf}} &
      {\includegraphics[width=47mm]{20_ahead.pdf}}\\
\end{tabular}\caption{Expected loss for different number of steps ahead in an example with 10 data points and the Six-hump Camel function. }\label{table:n_ahead}
\end{figure}

 \begin{center}
 \textcolor{red}{\textbf{Increasing the number of steps-ahead increased exploration in GLASSES}}
 \end{center}
 

\begin{table}[t!]
\begin{center}
\begin{tabular}{lcccccccc}
\hline
{} &     MPI &     GP-LCB &      EL &    EL-2 &    EL-3 &    EL-5 &  EL-10 &    GLASSES \\
\hline
SinCos  &  0.7147 &  0.6058 &  0.7645 &  \emph{0.8656} &  0.6027 &  0.4881 &  \emph{0.8274} &  \emph{\textbf{0.9000}} \\ 
Cosines           &  0.8637 &  0.8704 &  0.8161 &  \emph{0.8423} &  \emph{0.8118} &  0.7946 &  0.7477 &  \emph{\textbf{0.8722}} \\
Branin              &  0.9854 &  0.9616 &  \textbf{0.9900} &  0.9856 &  0.9673 &  0.9824 &  0.9887 &  0.9811 \\
Sixhumpcamel        &  0.8983 &  \textbf{0.9346} &  0.9299 &  0.9115 &  0.9067 &  0.8970 &  0.9123 &  0.8880 \\
Mccormick           & \textbf{0.9514} &  0.9326 &  0.9055 &  \emph{0.9139} &  \emph{0.9189} &  \emph{0.9283} &  \emph{0.9389} &  \emph{0.9424} \\
Dropwave            &  0.7308 &  0.7413 &  0.7667 &  0.7237 &  0.7555 &  0.7293 &  0.6860 &  \emph{\textbf{0.7740}} \\
%Beale &&& &&&&&\\ 
Powers              &  0.2177 &  0.2167 &  0.2216 &  \emph{0.2428} &  \emph{0.2372} &  \emph{0.2390} &  \emph{0.2339} &  \emph{\textbf{0.3670}} \\
Ackley-2 &  0.8230 &  \textbf{0.8975} &  0.7333 &  0.6382 &  0.5864 &  0.6864 &  0.6293 &  0.7001 \\
Ackley-5  & 0.1832&   0.2082&   0.5473&   \emph{0.6694}&  0.3582&   0.3744&   \emph{\textbf{0.6700}} &  0.4348\\ 
Ackley-10 &  0.9893 &  0.9864 &  0.8178 &   \emph{0.9900} &   \emph{0.9912} &   \emph{\textbf{0.9916}} &   \emph{0.8340} &   \emph{0.8567} \\
Alpine2-2 &  \textbf{0.8628} &  0.8482 &  0.7902 &  0.7467 &  0.5988 &  0.6699 &  0.6393 &  0.7807 \\
Alpine2-5  &  0.5221 &  0.6151 &  \textbf{0.7797} &  0.6740 &  0.6431 &  0.6592 &  0.6747 &  0.7123 \\
\hline
\end{tabular}\caption{Results for the average `gap' measure (5 replicates) across different functions.  EL-k: expect loss with $k$ steps ahead. MPI: maximum probability of improvement. GP-LCB: lower confidence bound criterion. }
\end{center}
\end{table}
 \begin{center}
 \textcolor{red}{\textbf{Non-myopic losses improve the results in practice }}
 \end{center}
 

      \end{block}



      %-- Block 3-2
      \begin{block}{Conclusions and future work}
\begin{itemize}
\item First non-myopic loss that allows taking into account dozens of future evaluations.
\item The loss compares well with current myopic acquisitions.
\item Challenge: making the optimisation of the loss more efficient.
\end{itemize}

      \end{block}

\begin{block}{References}
\begin{itemize}
\item[1] Michael Osborne. Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature. PhD thesis, University of Oxford, 2010.
\item[2] John P. Cunningham, Philipp Hennig, and Simon Lacoste-Julien. Gaussian probabilities and expectation propagation. arXiv:1111.6832 [stat], Nov 2011. arXiv: 1111.6832.
\item[3] Javier Gonz\'alez, Zhenwen Dai, Philipp Hennig, and Neil D Lawrence. Batch Bayesian optimization via local penalization. arXiv preprint arXiv:1505.08052, 2015.
\end{itemize}

\end{block}

    \end{column}%3

  \end{columns}
\end{frame}
\end{document}
