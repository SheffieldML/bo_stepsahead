\documentclass[15pt,serif,mathserif,final]{beamer}
\mode<presentation>{\usetheme{Lankton}}
\usepackage{amsmath,amsfonts,amssymb,pxfonts,eulervm,xspace}

\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
%\usepackage[demo]{graphicx}
%\usepackage{subcaption}
\usepackage{url}
%\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{nicefrac}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{amsfonts}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{color,soul,amsmath}
\usepackage{amssymb}
\usetikzlibrary{bayesnet}
\tikzstyle{connect}=[-latex]
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancybox}
\graphicspath{{./}{../../../bo_stepsahead/reports/diagrams/}}
\newcommand{\todiagrams}{../diagrams/}
\usepackage[orientation=portrait,size=custom,width=70,height=40,scale=.6,debug]{beamerposter}

\newcommand{\vp}{\vec{\phi}}
\newcommand{\vmu}{\vec{\mu}}
\newcommand{\vf}{\vec{f}}

\newcommand{\I}{\mathcal{I}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\E}{\mathbb{E}}

%\newcommand{\dataVector}{\textbf{y}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\vk}{\vec{k}}
\newcommand{\vL}{\vec{\Lambda}}
\newcommand{\xmin}{x_{\min}}
\newcommand{\pmin}{p_{\min}}
\newcommand{\fmin}{f_{\min}}

\newcommand{\pfmin}{p_{f_{\min}}}
\renewcommand{\vec}{\boldsymbol}
\newcommand{\fun}[1]{\mathsf{#1}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Id}{\vec{I}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\future}{\mathcal{F}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\argmin}{\operatorname*{arg\: min}}
\newcommand{\argmax}{\operatorname*{arg\: max}}
\newcommand{\chol}{\operatorname{\mathsf{C}}}

\newcommand{\reals}{\mathbb{R}}

\newcommand{\xst}{x_{\ast}}
\newcommand{\yst}{y_{\ast}}

\usepackage{xspace}
\newcommand{\acr}[1]{\textsc{#1}\xspace}
\newcommand{\gp}{\acr{gp}}
\newcommand{\dpp}{\acr{dpp}}
\newcommand{\us}{\acr{glasses}}
\newcommand{\direct}{\acr{direct}}
\newcommand{\lbfgs}{\acr{l-bfgs}}
\newcommand{\map}{\acr{map}}
\newcommand{\ep}{\acr{ep}}
\newcommand{\bo}{\acr{bo}}
\newcommand{\mpi}{\acr{mpi}}
\newcommand{\el}{\acr{el}}
\newcommand{\lcb}{\acr{gp-lcb}}


%-- Header and footer information ----------------------------------
%\newcommand{\footleft}{Funded by the BRIC-BBSRC  project No BB/K011197/1.}
%\newcommand{\footright}{\texttt{\{j.h.gonzalez}@sheffield.ac.uk} 
%\title{\Huge{GLASSES: Relieving The Myopia Of Bayesian Optimisation}}
%\author{Javier Gonz\'{a}lez$^{1,2}$ \quad Michael Osborne$^{2}$  \quad  Neil D. Lawrence$^{1}$}
%\institute{$^1$Department of Computer Science, $^2$Department of Chemical and Biological Engineering\\
%University of Sheffield, UK}
%-------------------------------------------------------------------
\definecolor{mycolor}{rgb}{0, 0, 1}
\definecolor{trolleygrey}{rgb}{0.5, 0.5, 0.5}
%-- Main Document --------------------------------------------------

\begin{document}

\begin{frame}{}


\begin{columns}[t]

%-- Column 1 ---------------------------------------------------
\begin{column}{0.32\linewidth}

%-- Block 1-1
\begin{block}{Summary}
\begin{itemize}
\item We present \textcolor{\us}: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. 
\item \textcolor{\us} is a \textcolor{mycolor}{non-myopic loss for Bayesian Optimisation} that permits the consideration of dozens of evaluations into the future.
\item We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests. 
\item Code available in the GPyOpt package.
\end{itemize}
\end{block}


%-- Block 1-2
\begin{block}{Problem definition}
\end{block}


 %-- Block 1-3
\begin{block}{Myopic loss Bayesian Optimisation}

\begin{figure}[t!]
\centering
\begin{tikzpicture}

    % first row
    \node[obs] (D0) {$\dataSet_0$};
    \node[latent, right=of D0, xshift=1.2cm] (D1) {$\dataSet_1$};
    \node[draw=none, right=of D1, xshift=1.2cm] (Ddots) {$\ldots$};
    \node[latent, right=of Ddots, xshift=1.2cm] (Dn) {$\dataSet_n$};

    % second row
    \node[det, below=of D0, xshift=1.2cm] (xst) {$\xst$};
    \node[latent, right=of xst, xshift=1.2cm] (x2) {$x_2$};
    \node[latent, right=of x2, xshift=4.8cm] (xn) {$x_n$};

    % third row
    \node[latent, below=of xst] (yst) {$\yst$};
    \node[latent, below=of x2] (y2) {$y_2$};
    \node[draw=none, right=of y2, xshift=1.2cm] (ydots) {$\ldots$};
    \node[latent, below=of xn] (yn) {$y_n$};

    % Connect the nodes
    \path 
        (D0) edge [connect] (D1)
        (D0) edge [connect] (xst)
        (xst) edge [connect] (yst)
        (xst) edge [connect] (D1)
        (yst) edge [connect, bend right=20] (D1)
        (yst) edge [allconnected] (y2)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (y2) edge [allconnected] (ydots)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (ydots) edge [allconnected] (yn)

        (Ddots) edge [connect] (Dn)
        (Dn) edge [connect] (xn)
        (xn) edge [connect] (yn)        
        ;
\end{tikzpicture}
\caption{
    A Bayesian network describing the $n$-step lookahead problem. The shaded node ($\dataSet_0$) is known, and the diamond node ($\xst$) is the current decision variable. All $y$ nodes are correlated with one another under the \gp model. Note that the nested maximisation problems required for $x_i$ and integration problems required for $y_*$ and $y_i$ (in either case for $i=2,\ldots, n$) render inference in this model prohibitively computationally expensive.
}
\end{figure}

\end{block}
\end{column}%1

%-- Column 2 ---------------------------------------------------
\begin{column}{0.32\linewidth}



%-- Block 2-3
\begin{block}{Model for genes efficiency}
\end{block}

%-- Block 2-2
\begin{block}{Acquisition and evaluation functions}
\end{block}

\end{column}%2



%-- Column 3 ---------------------------------------------------
\begin{column}{0.32\linewidth}

%-- Block 3-1
\begin{block}{Results}
\end{block}



%-- Block 3-2
\begin{block}{Conclusions and future work}
\end{block}

\begin{block}{References}

\end{block}

\end{column}%3
\end{columns}
\end{frame}

\end{document}
