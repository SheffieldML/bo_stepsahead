\documentclass[twoside]{article}
\usepackage{aistats2016}


\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{nicefrac}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{color,soul,amsmath}

\usetikzlibrary{bayesnet}
\tikzstyle{connect}=[-latex]
\tikzstyle{allconnected}=[line width=0.1cm]

%\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{tikzpicture}
%\usepackage{savetrees}

\def\bbbr{{\rm I\!R}}
\newcommand{\vp}{\vec{\phi}}
\newcommand{\vmu}{\vec{\mu}}
\newcommand{\vf}{\vec{f}}

\newcommand{\I}{\mathcal{I}}

\newcommand{\vm}{\vec{m}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vr}{\vec{r}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}
\newcommand{\vY}{\vec{Y}}
\newcommand{\vX}{\vec{X}}
\newcommand{\bx}{\textbf{x}}
\newcommand{\bz}{\textbf{z}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\by}{\textbf{y}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\vk}{\vec{k}}
\newcommand{\vL}{\vec{\Lambda}}
\newcommand{\xmin}{x_{\min}}
\newcommand{\pmin}{p_{\min}}
\newcommand{\fmin}{f_{\min}}
\newcommand{\hpmin}{\hat{p}_{\min}}
\newcommand{\hqmin}{\hat{q}_{\min}}
\newcommand{\pfmin}{p_{f_{\min}}}
\renewcommand{\vec}{\boldsymbol}
\newcommand{\fun}[1]{\mathsf{#1}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Id}{\vec{I}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\bk}{\textbf{k}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\bK}{\textbf{K}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\argmin}{\operatorname*{arg\: min}}
\newcommand{\argmax}{\operatorname*{arg\: max}}
\newcommand{\chol}{\operatorname{\mathsf{C}}}

\newcommand{\data}{\mathcal{D}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\xst}{x_{\ast}}
\newcommand{\yst}{y_{\ast}}

\usepackage{xspace}
\newcommand{\acr}[1]{\textsc{#1}\xspace}
\newcommand{\gp}{\acr{gp}}
\newcommand{\dpp}{\acr{dpp}}
\newcommand{\us}{\acr{glasses}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{proof}{Proof}
\newtheorem{conjecture}{Conjecture}



\graphicspath{{./},{./figs/}}

% If your paper is accepted, change the options for the package
% aistats2016 as follows:
%
%\usepackage[accepted]{aistats2016}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{GLASSES: Relieving The Myopia Of Bayesian Optimisation}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
    We present \us: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. 
    The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. 
    Our novel algorithm, \us, permits the consideration of dozens of evaluations into the future. 
    We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests. 
\end{abstract}
\section{Introduction} % (fold)
\label{sec:introduction}

Global optimisation is core to any complex problem where design and choice play a role. 
Within Machine Learning, such problems are found in the tuning of hyperparameters \cite{Snoek*Larochelle*Adams_2012}, sensor selection \cite{Garnett*Osborne*Roberts_2010} or experimental design \cite{martinez-cantin_bayesian_2009}. 
Most global optimisation techniques are myopic, in considering no more than a single step into the future. 
Relieving this myopia requires solving the \emph{multi-step lookahead} problem: the global optimisation of an function by considering the significance of the next function evaluation on function evaluations (steps) further into the future. 
It is clear that a solution to the problem would offer performance gains.
For example, consider the case in which we have a budget of two evaluations with which to optimise a function $f(x)$ over the domain $\sX = [0, 1] \subset \reals$. 
If we are strictly myopic, our first evaluation will likely be at 
$x=\nicefrac{1}{2}$, and our second then at only one of $x=\nicefrac{1}{4}$ and $x=\nicefrac{3}{4}$. 
This myopic strategy will thereby result in ignoring half of the domain $\sX$, regardless of the second choice. 
If we adopt a two-step lookahead approach, we will select function evaluations that will be more evenly distributed across the domain by the time the budget is exhausted. 
We will consequently be better informed about $f$ and its optimum.

There is a limited literature on the multi-step lookahead problem.
\cite{osborne_gaussian_2009} perform multi-step lookahead by optimising future evaluation locations, and sampling over future function values. 
This approach scales poorly with the number of future evaluations considered, and the authors present results for no more than two-step lookahead.
\cite{Marchant*Ramos*Sanner*2014} reframe the multi-step lookahead problem as a partially observed Markov decision process, and adopt a Monte Carlo tree search approach in solving it. 
Again, the scaling of the approach permits the authors to consider no more than six steps into the future. 

There is a clear link between the multi-step lookahead problem and that considered in the literature as \emph{batch} Bayesian optimisation. 
The two problems are distinct but related: the multi-step lookahead problem requires the challenging marginalisation over unknown future evaluation \emph{locations}, in addition to the unknown future evaluation \emph{values} also marginalised by batch approaches. 
Similarly to the state-of-the-art in multi-step lookahead, the batch literature provides only poor scaling with the number of evaluations.
\cite{Ginsbourger2009} present results for no more than six simultaneous function evaluations.
\cite{Azimi2011,Azimi2012} use the surrogate model for $f$ to generate `fake' observations and avoid the marginalization step. 
This produce a large accumulation of errors that does not allow the use of these techniques for the collection of large batches.

We propose an algorithm, \us, that provides scaling superior to existing alternatives.

\subsection{Bayesian Optimisation with one step look-ahead} % (fold)
\label{sec:bayesian_optimisation}




Let $f: {\mathcal X} \to \bbbr$ be well behaved function defined on a compact subset ${\mathcal X} \subseteq \bbbr^d$. We are interested in solving the global optimization problem of finding $\bx_{M} = \arg \min_{\bx \in {\mathcal X}} f(\bx)$. We assume that $f$ is a \emph{black-box} from which only perturbed evaluations of the type $y_i = f(\bx_i) + \epsilon_i$, with $\epsilon_i \sim\mathcal{N}(0,\sigma^2)$, are  available. The goal is to make a series of  evalu


ations $\bx_1,\dots,\bx_N$ of $f$ such that the the minimum of $f$ is evaluated as soon as possible
.

Assume that $n$ points have been gatherd so far, having a dataset $\data_0 = {(\bx_i,y_i)}_{i=1}^n$. In BO the first thing to do before sampling a new location is to build a probabilistic mode for $f$, tipically a Gaussian Process (GP) $p(f) = \mathcal{GP}(\mu; k)$ with mean function $\mu$ and
positive-definite covariance function $k$ with parameters $\theta$. The posterior distribution of $f$ is also a GP under Gaussian likelihoods. Let us denote by $\I_0$ the current available infomation: the conjunction of $\data_0$, the model parameters, $\theta$ and the model likelihood. The predictive distribution for $y_*$ at $\bx_* \in {\mathcal X}$ is Gaussian with mean
posterior mean and variance given by
$$\mu(y_{*}|\I_0) = \bk_{\theta}(\bx_*)^\top[\bK_{\theta} + \sigma^2 \textbf{I}]^{-1}\by,$
and
$$\sigma^2(y_*|\I_0)=k_{\theta}(\bx_*,\bx_*)-\bk_{\theta}(\bx_*)^\top[\bK_{\theta}+\sigma^2 \textbf{I}]^{-1}\bk_{\theta}(\bx_*),$$
where $\bK_{\theta}$ is the matrix such that $(\bK_{\theta})_{ij}=k_{\theta}(\bx_i,\bx_j)$,  $\bk_{\theta}(\bx_{*}) = [k_{\theta}(\bx_1,\bx_{*}),\dots,k_{\theta}(\bx_n,\bx_{*})]^\top$ \cite{Rasmussen:2005:GPM:1162254} and $\bx_{*}$ is the point where the GP is evaluated.  

$$\lambda(y)= \left\{ \begin{array}{lcl}
y;             & \mbox{ if } & $y < \eta$  \\
 $\eta;$ & \mbox{ if } & $ y \leq \eta$\\
\end{array}
\right.$$


\cite{Lizotte_2008}
\cite{Jones_2001}
\cite{Snoek*Larochelle*Adams_2012}
\cite{Brochu*Cora*DeFreitas_2010}

% section bayesian_optimisation (end)

% section introduction (end)

\begin{figure*}
\centering
\begin{tikzpicture}

    % first row
    \node[obs] (D0) {$\data_0$};
    \node[latent, right=of D0, xshift=1.2cm] (D1) {$\data_1$};
    \node[draw=none, right=of D1, xshift=1.2cm] (Ddots) {$\ldots$};
    \node[latent, right=of Ddots, xshift=1.2cm] (Dn) {$\data_n$};

    % second row
    \node[det, below=of D0, xshift=1.2cm] (xst) {$\xst$};
    \node[latent, right=of xst, xshift=1.2cm] (x2) {$x_2$};
    \node[latent, right=of x2, xshift=4.8cm] (xn) {$x_n$};

    % third row
    \node[latent, below=of xst] (yst) {$\yst$};
    \node[latent, below=of x2] (y2) {$y_2$};
    \node[draw=none, right=of y2, xshift=1.2cm] (ydots) {$\ldots$};
    \node[latent, below=of xn] (yn) {$y_n$};

    % Connect the nodes
    \path 
        (D0) edge [connect] (D1)
        (D0) edge [connect] (xst)
        (xst) edge [connect] (yst)
        (xst) edge [connect] (D1)
        (yst) edge [connect, bend right=20] (D1)
        (yst) edge [allconnected] (y2)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (y2) edge [allconnected] (ydots)

        (D1) edge [connect] (Ddots)
        (D1) edge [connect] (x2)
        (x2) edge [connect] (y2)
        (x2) edge [connect] (Ddots)
        (y2) edge [connect, bend right=20] (Ddots)
        (ydots) edge [allconnected] (yn)

        (Ddots) edge [connect] (Dn)
        (Dn) edge [connect] (xn)
        (xn) edge [connect] (yn)        
        ;
\end{tikzpicture}
\caption{
    A Bayesian network describing the $n$-step lookahead problem. The shaded node ($\data_0$) is known, and the diamond node ($\xst$) is the current decision variable. All $y$ nodes are correlated with one another under the \gp model.
}
\label{fig:bayes_net}
\end{figure*}


\subsection{Looking many steps ahead: Challenge and contribution of this work}


The goal of any batch criterion is to mimic the decisions that would be made under the equivalent (optimal) sequential policy: Consider the choice of selecting $\bx_{t,k}$, the $k$-th element of the $t$-th batch. Under a sequential policy, in which the evaluations of $f$ at all locations prior to $\bx_{t,k}$ are available, the decision is to take $\bx_{t,k}$ as the maximizer of $\alpha(\bx;\I_{t,k-1})$. In the batch case, the decision about where to collect $\bx_{t,k}$ has to incorporate the uncertainty about the locations  $\bx_{t,1},\dots,\bx_{t,k-1}$, and the outcomes of the evaluation of $f$ there. Iteratively marginalizing these sources of uncertainty gives
\begin{equation}\label{eq:optimal_batch}
\Lambda_n(\bx^\star|\I_0 )= \int \lambda(y_n) \prod_{j=1}^{n}p(y_{j}|\bx_{j},\I_{j-1}) p(\bx_{j}|\I_{j-1} )\text{d}\bx_{j} \text{d}y_{j},\\
\end{equation}
where $p(y_{t,j}|\bx_{t,j},\I_{t,j-1})= \mathcal{N} \left(y_{t,j};\mu_n(\bx_{t,j};\I_{t,j-1}),\sigma_n^2(\bx_{t,j};\I_{t,j-1} ) \right)$ is the predictive distribution of the GP at $\bx_{t,j}$ when a total of $n$ points are available and $p(\bx_{t,j}|\I_{t,j-1}) = \delta (\bx_{t,j} - \arg \max_{\bx \in {\inputSpace}} \alpha(\bx;\I_{t,j-1}) )$ reflects the optimization step required to obtain $\bx_{t,j}$ after the evaluations of $f$ at previous batch-elements have been marginalized. %In Eq. (\ref{eq:optimal_batch}), the `integration' with respect to $\bx_{t,k}$ is explicitly kept as a maximization step, so it is straightforward to identify this expression with the sequential policy when $n_b=1$.



The optimization in Eq.~(\ref{eq:optimal_batch}) is intractable even for small batch-sizes, due to the optimization-marginalization loop required to obtain $\bx_{t,k}$.




 \section{The {\us}
 Algorithm}

\subsection{Predicticting BO future steps}


\subsection{Computing the Expected Loss}
Our goal is to compute $E_{p(\by)} [\min (\by,\eta)]$ for $\by=\{y_1,\dots,y_n\}$ and $p_0(\by) \sim \N(\by; \mu, \Sigma)$. This  approximates the n-steps ahead expected loss $\Lambda_n(\bx_*)$. The goal of this section is to write  $E_{p(\by)} [\min (\by,\eta)]$ in a way that it is suitable to be computed by Expectation Propagation. Next proposition will do the work (I think).

\begin{proposition}
It holds that 
\begin{eqnarray}\nonumber
E_{p(\by)} [\min (\by,\eta)] & = & \sum_{j=1}^n  \int_{\IR^n} y_j \prod_{i=1}^n t_{j,i}(\by) \N(\by; \mu, \Sigma) d \by \\ \nonumber
&+& \eta\int_{\IR^n} \prod_{i=1}^nh_i(\by) \N(\by; \mu, \Sigma) d\by \label{eq:expected_loss}
\end{eqnarray}
where  $h_i(\by) = \mathbb{I}\{y_i>\eta\}$ and
$$t_{j,i}(\by)= \left\{ \begin{array}{lcl}
\mathbb{I}\{y_j \leq\eta\} & \mbox{ if } $ i=j$ \\
  \\
 \mathbb{I}\{ 0 \leq y_i-y_j \} &   \mbox{otherwise.} 
\end{array}
\right.$$

\end{proposition}



All the elements in (\ref{eq:expected_loss}) can be rewritten in a way that can be computed using EP but the work in \cite{Cunningham*Hennig*Lacoste-Julien_2011}. 

\begin{itemize}
\item The second term is a Gaussian probability on unbounded polyhedron in which the limits are aligned with the axis.  
\item The first term requires some more processing but it is still computable under the assumptions in \cite{Cunningham*Hennig*Lacoste-Julien_2011}. Let $\bw_j$ the $jth$ canonical vector. Then we have that
\begin{eqnarray}
\int_{\IR^n} y_j \prod_{i=1}^n t_{j,i}(\by) \N(\by; \mu, \Sigma) d \by & = & \bw^T  \int_{\IR^n} \by \prod_{i=1}^n t_{j,i}(\by) \N(\by; \mu, \Sigma) d \by \\
& = & \bw^T E[\by] z_j
\end{eqnarray}
where the expectation is calculated over the normalized distribution over $P_j$, the one EP approximates with $q(\by)$, and for $z_j$ being the normalizing constant 
  $$z_j= \int_{\IR^n} \prod_{i=1}^n t_{j,i}(\by) \N(\by; \mu, \Sigma) d \by$$  
Because EP does moments matching, both the normalizing constant and the expectation are available.
\end{itemize}



\subsection{Algorithm}


\begin{algorithm*}[t!]
   \caption{Decision process of the \us algorithm.}
   \label{alg:glasses}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{D}_{0} = \{(\textbf{x}_0, y_0)\}$, number of remaining evaluations ($n$).
   \STATE Fit a \gp with kernel $k$ to $\mathcal{D}_{0}$.
   \STATE Select $\bx_{1*},\dots,\bx_{r*}$ representer points of the loss.
   \FOR{$j=1$ {\bfseries to} $r$ }
   \STATE Take $s$ samples from a conditional n-\dpp of kernel $k$ given $\bx_{j*}$.
   \STATE Approximate the expected loss at $\bx_j^*$ for the $s$ samples computing $E [\min (\by,\eta)]$.
  \STATE Average the expected loss for the $s$ samples and obtain $\tilde{\Lambda}_n(\bx_j^*)$.
   \ENDFOR
\STATE Approximate $\Lambda_n(\bx_*)$ fitting a $\gp_2$  to $\{(\bx_{j*}, \tilde{\Lambda}_n(\bx_{j*})\}_{j=1}^r$ with posterior mean $\mu_2$.
   \STATE \textbf{Returns}: New location at $\arg \min_{x \in \mathcal{X}} \left\{\mu_2(\bx)\right\}$.  
\end{algorithmic}
\end{algorithm*}

\section{Results}


\section{Conclusions}

\bibliographystyle{plain}
\bibliography{bib_glasses}

\clearpage
\setcounter{section}{0}
\setcounter{equation}{0}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\theequation}{S.\arabic{equation}}

\begin{center}
{\LARGE  Supplementary materials for:\\
`GLASSES: Relieving The Myopia Of Bayesian Optimisation"}
\end{center}
\begin{center}
\textbf{Authors here}
\end{center}

\section{Proofs}

\begin{proof}
Denote by 
\begin{eqnarray}\nonumber
E_{p(\by)} [\min (\by,\eta)] & = & \int_{\IR^n} \min (\by,\eta)  \N(\by; \mu, \Sigma) d\by\\ \nonumber
& = & \int_{\IR^n - (\eta,\infty)^n } \min (\by)  \N(\by; \mu, \Sigma) d\by + \int_{(\eta,\infty)^n} \eta  \N(\by; \mu, \Sigma) d\by  \nonumber
\end{eqnarray}

The first term can be written as follows:

\begin{equation}
 \int_{\IR^n - (\eta,\infty)^n } \min (\by)  \N(\by; \mu, \Sigma) d\by  =    \sum_{j=1}^n \int_{P_j} y_j \N(\by; \mu, \Sigma) d \by \nonumber
\end{equation}\nonumber

where $P_j := \{ \by \in\IR^n - (\eta,\infty)^n  : y_j \leq y_i,\,\, \forall i \neq j \}$. We can do this because the regions $P_j$ are disjoint and it holds that $\cup_{j=1}^{n}P_j = \IR^n - (\eta,\infty)^n $.  Also, note that the $\min(\by)$ can be replaced within the integrals since within each $P_j$ it holds that $\min(\by) = y_j$. Rewriting the integral in terms of indicator functions we have that
\begin{eqnarray}\label{eq:term1}
 \sum_{j=1}^n \int_{P_j} y_j \N(\by; \mu, \Sigma) d \by   =  \sum_{j=1}^n  \int_{\IR^n} y_j \prod_{i=1}^n t_{j,i}(\by) \N(\by; \mu, \Sigma) d \by 
\end{eqnarray}

where $t_{j,i}(y) =\mathbb{I}\{y_i \leq\eta\}$ if $j=i$ and $t_{j,i}(y) =\mathbb{I}\{y_j \leq y_i \}$ otherwise.

The second term can be written as
\begin{equation}\label{eq:term2}
 \int_{(\eta,\infty)^n } \eta  \N(\by; \mu, \Sigma) d\by = \eta\int_{\IR^n} \prod_{i=1}^nh_i(\by) \N(\by; \mu, \Sigma) d\by
\end{equation}
where $h_i(\by) = \mathbb{I}\{y_i>\eta\}$.  Merge (\ref{eq:term1}) and (\ref{eq:term2}) to conclude the proof.
 
\end{proof}


\end{document}