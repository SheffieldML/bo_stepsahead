import numpy as np
import GPy
from GPyOpt.core.optimization import wrapper_lbfgsb
from .util.general import samples_multidimensional_uniform, reshape
from .dpp_samplers.dpp import sample_dual_conditional_dpp
from .quadrature.emin_epmgp import emin_epmgp

class acq_GPGOMSA:
	'''
	Wrapper class for a GPy model to minimize the posterion mean
	'''
	def __init__(self,model):
		self.model = model

	def f(self,x):
		return model.predict(x)[0]

	def d_f(self,x):
		dmdx = model.predictive_gradients(x)
		return dmdx[:,:,0]

class GPGOMSA:
	'''
	Class to run Bayesian Optimization with multiple steps ahead
	'''

	def __init__(self,f,bounds,X,Y):
		self.input_dim = len(bounds)
		self.X = X
		self.Y = Y
		self.kernel = GPy.kern.RBF(self.input_dim, variance=.1, lengthscale=.1)  + GPy.kern.Bias(self.input_dim)
		self.model  = GPy.models.GPRegression(X,Y,kernel=kernel)
		self.model.optimize()

	def run_optimization(max_iter,n_ahead=None, eps= 10e-6):

		# Check the number of steps ahead to look at
		if n_ahead==None:
			self.n_ahead = max_iter
		else:
			self.n_ahead = n_ahead

		# initial stop conditions
		k = 1
		distance_lastX = np.sqrt(sum((X[X.shape[0]-1,:]-X[X.shape[0]-2,:])**2))

		while k<max_iter and distance_lastX > eps:

			# Evaluate the loss ahead acquisition function in a set of representer points
			x_acq = samples_multidimensional_uniform(bounds,500)
			y_acq = loss_nsahead(x_acq,n_ahead,model,bounds)

			# Build the acquisition: based on a model on the representer points
			kernel_acq      = GPy.kern.RBF(input_dim, variance=.1, lengthscale=.1)  + GPy.kern.Bias(input_dim)
			model_acq   	= GPy.models.GPRegression(x_acq,y_acq,kernel=kernel_acq)
			model_acq.optimize()

			#Create object to optimize
			acq = acq_GPGOMSA(model_acq)

			# Optimize the posterior mean on the model and find the best location
			samples = samples_multidimensional_uniform(bounds,500)
			x0 =  samples[np.argmin(acq.f(samples))]
			x_new = wrapper_lbfgsb(acq.f, acq.df, x0, bounds)

			# Augment the dataset
			self.X = np.vstack((X,X_new))
			self.Y = np.vstack((Y,f(X_new)))

			# Update the model
			self.model.X = X
			self.model.Y = Y
			self.model.optimize()

			# Update steps ahead if needed
			if n_ahead==None:
				self.n_ahead -=1


def loss_nsahead(x, n_ahead, model, bounds):
    x = reshape(x,model.X.shape[1]) 
    n_data = x.shape[0]
    
    # --- fixed options
    num_data       = 500              # uniform samples
    n_replicates    = 10             # dpp replicates
    q             = 50             # truncation, for dual dpp

    # --- get values
    losses        = np.zeros((n_data,n_replicates))
    Y             = model.Y
    eta           = Y.min()
    
    X0            = samples_multidimensional_uniform(bounds,num_data)
    set1          = [1]
    
    for k in range(n_data):
        X             = np.vstack((x[k,:],X0))
       
        # --- define kernel matrix for the dpp
        L   = model.kern.K(X)
    
        for j in range(n_replicates):
            # --- take a sample from the dpp (need to re-index to start from zero)
            dpp_sample = sample_dual_conditional_dpp(L,set1,q,n_ahead)
            dpp_sample = np.ndarray.tolist(np.array(dpp_sample)-1)
          
            # evaluate GP at the sample and compute full covariance 
            m, K       = model.predict(X0[dpp_sample,:],full_cov=True)
       
            # compute the expected loss
            losses[k][j]  = emin_epmgp(m,K,eta)

        return losses.mean(1).reshape(n_data,1)




